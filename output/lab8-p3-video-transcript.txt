 All right, hello, everyone. Welcome back to each 370 discussion. This is the first discussion after spring break in the exam, but today we're putting that all behind us and looking forward to the rest of the course. So far we've been concerned a lot with the ISA design in the course, but now we're looking forward to taking some basic data path implementations and making them faster, but still making them work. So that's what we're concerned with for pipelining and Project 3 and past semesters. This discussion has been less focused on Project 3 and more focused on just pipelining in general from lecture, but I think some modifications I've made to the slides for this week will help a lot with Project 3. Before we get started, I would ask you to fill out this feedback form, even if you miss the cutoff for the official U of M feedback, you can go to this link or scan the QR code and give me some feedback that I can use for the rest of the term and in future semesters. And if we get 50% of the class to fill it out by Tuesday, which is more than 24 hours from the time of this recording, then everyone will get an extra late day. I would love to see that happen last time I heard last Thursday, we were at about 35%. So I'm really hoping that we get this extra late day pretty soon. That'd be really cool. Looking ahead, the second half of the term, we'll be very similar to the first half of the term. Our projects are still going to be doing Thursdays and our homeworks are still going to be doing Mondays. And for Project 3, it's unlike Projects 1 and 2, which are broken up into a bunch of different pieces, Projects 3 and 4 are just one submission for the entire project. But we're trying to break that up a little bit for Project 3. So we have this thing do this upcoming Thursday, which we call a Project 3 checkpoint. And that's not going to be a grade, at least for this semester. But you submit your project code, which will be modified for the final Project 3 submission. And it's just a basic working version of your Project 3 code. So we're asking you to submit that so that you can get some feedback on whether or not you have the basics down. So that you can fix those before moving on to the trickier implementations for the rest of Project. And today in this discussion, I'll talk more about what exactly that checkpoint is testing for. So as you can get that all working before moving on to the rest of Project 3. I love showing these graphs as you know, but here's a distribution from Project 2C just a couple weeks ago. So of course, if you start later, you're less likely to receive a good score on the project just because of how workflows work. And here's that same graph from Project 3 from last term. You can see it's quite spread out, especially in the last days and Project 3 in particular. In my opinion, it's the toughest project. And that's mainly because of the instructor by eSolutions. A lot of people will get like 98 96. Just because the bugs are so hard to catch, even if you get all of the implementation features correct. So I highly highly highly recommend spending extra time on your testing for this project. That will get you just over the line to that 100 that everyone wants. Or it might help you in other places, you never know. So there's our distribution. So start early on Project 3. That's why we have the checkpoint. And so, even though it's not for grade, it is to help you start earlier and get some basic features done before final submission time comes. So here's what we'll be doing today. I'll introduce the pipeline, the concept of pipelining as it pertains to processors. And it's really going to be a combination of our single cycle, that we worked on before the exam. And then those week's leading up to spring break. of the last two discussions for how we talked about those, how we extended those. And so today we're not going to be really focusing on adding new instructions to a pipeline we're more concerned today about just where the data is going in general. After that, I do want to talk about the differences between the the data path that we have in the project versus what we've introduced in lecture. There are some differences that always trick people up because they are actually confusing. So I'll explain this today. And then we'll just look at hazards. What are they, how we fix them, how we'll fix them in the project, and then we'll be done. So like I said, the pipeline is a combination of our single cycle and multi cycle. And if we think back to the idea of the single cycle, the single cycle was all about completing one instruction every clock cycle. So every clock cycle, we would pretty much start with our PC. That was like our input argument. And then that would propagate through our instruction memory to give us the machine code. And that would go through the register file to give us the values that we're working with. And then we'd send things to the LU to make calculations. We might go through memory to get more values or modify values in memory. And then in we'd write it back to our register file, the updated value that we've calculated that cycle. And so all that takes a very long propagation. And so we had very high clock periods. But we knew that every single cycle, exactly one instruction could get executed. And so we didn't really have to do any extra sort of steps beyond that one clock cycle in order to execute an instruction. The multi cycle was sort of an extension of that because we saw that this high clock period on a single cycle data path was really just killing us in terms of performance. And so we were like, what if we split up all these different steps that we have to do for each instruction and do those in one clock period? So step one, step two, step three, step four, and step five if we're talking about LW instructions. And in theory, by partitioning that out, that should reduce the runtime for a single instruction. But we saw that that wasn't always the case because our really high latency components still made all the other calculations and components take a long time. And so sometimes our execution time increased from the multi cycle. If we weren't just using data paths or benchmarks with a bunch of norms. And for a deeper in depth example, we'll look at a specific benchmark next week, where we'll go through the single cycle and the multi cycle and a bunch of different implementations of the pipeline and see how that is actually true. Even though we wanted to partition things up and make it faster, it actually kind of made things slower, which is not intuitive. So now we're moving on to pipelining and we're pretty much taking the low CPI, the idea that we're completing one instruction every clock cycle and a relatively low clock period, and we're combining them together to get the best of both worlds and get better performance in the end. And so before we look specifically at the pipeline data path, I do want to show you my pictures just remind us of our single cycle and multi cycle. So here's our single cycle. And we started essentially with the PC, like I said. So here's our start point. And we went through instruction memory, the register file and the LU and data memory. And then it all went back and pretty much our endpoint was writing back to the register file. At least if we're talking about an LW and some other instructions. So the longest instruction that LW would take that entire propagation time. And that was what was driving our clock cycle. Now here's our multi cycle. And our multi cycle, you can't really visualize a single instruction happening just by looking at the data path. To visualize an instruction happening in the data path, we have to look at our little state machine for the multi cycle. So we had our instruction machine code fetch. So all right, that gets your instruction machine code. So we would fetch that from instruction memory as our first cycle for every single instruction. Then we would decode, which is really just like reading the register file. So let's see read register file. Then after that, we typically did some sort of LU calculation. So for ad, it was an addition for NOR. It was a bit wise NOR. LW is an addition between our offset and our register A. And it's same for SW. And then for BQ, we are doing the equality check. So all these cycles, cycles 24, 6, 9, and 11, as on the side. The third cycle of those five instructions was all stuff having to do with the ALU. So looking back, so we have this ALU. And that's what we were doing with the third cycle typically. And then after that, it gets a little bit hairy as to what's going on. So for cycle 7 and 10, which are only for LW and SW, we were accessing memory. And then for ad NOR and LW, those final cycles, we were writing back to the register file. So we were writing the register file. And then DEQ didn't do anything that was particularly string-uous in terms of propagation delay. But in the last cycle of BQ, we were updating the PC. Right. And so as you can see, I've kind of partitioned all of these cycles out into these very helpful boxes. And I'll add some others for the constant states that we have up at the top. And so this is sort of the way that we can visualize what components are being used across side. So with a single cycle, we could just look at the propagation delay where things were signals are starting and where they're ending the steps that they're going through there. But for the multicycle, we need this little state machine diagram to tell us what's going on as it pertains to time. And so we can see that the general steps here are as follows. So we had a fetch for instruction machine code. We had a decode. We had some sort of ALU operation, which I'm going to call execution. Then depending on what's going on, let's say we could either write the register or access memory or update the PC, I'm just going to put the write register after memory, since that definitely has to happen for LW. So we might access memory, and then we might write to the register file. And then of course, we had the PC updating over here or doing something, I guess we could put that into a write register, but there's no reason to not put it with memory since they're kind of distinct. So I'll put that here. And then we see that we kind of have five general steps to executing any of the instructions at least for LW. So if we're looking at other ISAs, we might have more complex basic steps here. But that just kind of depends on the ISA, and I would say in general, this sort of pipeline execution process is going to be relatively constant. The idea is what we're trying to teach here, which again is what makes LCHK really nice for this class. So this is the general flow of an instructions execution. And we're going to use this in our pipeline part of it. So the idea of the pipeline, we see that in each of these five sorts of stages, if we're using, if one instruction is sort of in one stage, then all the other stages are empty. And so since everything's kind of distinct, why don't we just have different instructions in different stages at the same time. So then we're always using all of our components as much as possible. So that means that memory is always in use or in theory. Our ALU is always doing something. There's always something in each of those stages. And so we're maximizing all of our components, which maximizes our power usage among other things. And so that means that even though each instruction is doing about five stages to complete, we're executing five instructions at a time. And so our CPI comes to be about equal to one with exceptions because of hazards. And so we'll get into that today. So that's the idea with the pipeline. So here's our actual data path. And you can see each of these stages. So we have our instruction fetch, which I will abbreviate to IF. And that's reading instruction memory. So starting with our PC, just like the single cycle, that's our beginning point. And then we have our ID stage or decode. So fetch decode. And then I said execution, which is what we'll call ALU operations. So execution. Then we have some sort of memory stage, which we abbreviate MEM or MEM. And then we have a writeback stage, which specifically writes back to registers. And I'll abbreviate that WB. You could also call this a commit stage. If you're familiar with that terminology, we don't really use that in this class. And so here are the five stages. And all the stages, just like one cycle of the multi cycle or one cycle, the same cycle, are all they're all going to be combinational logic. So all the wires and things and components that you see within these stages have been done out here. That's our combinational logic. But in order to use our clock appropriately, we still need to have some sort of sequential logic. And that's going to be represented by these different pipeline registers, which are the gray and the pinkish little boxes that we have between each stage. And they're going to act as blocks, these little data holders that will stop the propagations from one stage to the next. And then on the next cycle, it will allow those values to propagate to the next stage. So we're really creative with names in computer science and computer engineering. So we're just going to call these pipeline registers by the names of whatever stages they're in between. So for example, this first pipeline register is the IFIE pipeline register, real creative. And each pipeline register is going to have exactly the registers that we need for the in between values between stages. It won't have anything really extra. So that will be helpful for us in calculating exactly what we need to each stage. We have all the data we need and none of what we don't. All right, so let's keep going here. Now I want to talk specifically about the projects and how we implement this in the project. So the project is going to be somewhat similar to Project One S for you're building the simulator. And you can think of Project One S as probably like a like a single cycle processor. Or every iteration of this giant wow loop that you have is one clock cycle. And it's doing all these things sequentially. That's all the combinational logic propagating through. But for Project Three it's going to be similar. We'll have another wow loop. And you'll just execute the different stages stage by stage. So your project code will look like this. So you have this wow loop. And I don't remember the exact condition, but it's provided for you to start a code. And every time you execute this wow loop, you'll first do all your fetch stuff. And then you'll do your decode stuff. Then you'll do your execution. And then your memory operations. And then you're right back. And then you'll be done. And you'll go back to the beginning. Now in our actual processor, all these things, all these different stages are happening concurrently. They're happening all at the same time. But because we're doing this in code and not in some sort of like a visualizer with wires and much things. Or maybe we're not actually doing it with wires in our hands. We do have to do some things sequentially. And so I highly recommend just following that flow, which is provided for you in the start code. Just doing them a bit by bit. And that is going to bring us some issues which we will fix as we go along in the project. So to represent each of the actual stages, that's going to be a bunch of instructions, lines of C code that you will write. But to represent the pipeline registers, we've given you a giant state struct, just like project one, project one S specifically. And the state struct contains your registers, your memory. But this time, it also includes all these separate structs, substrucs if you will. You can you can practice your your padding calculations that we did last term or less half of the term. But we've given you a struct for each of these pipeline registers within your giant state struct. And so that's going to hold your variables and things that you use in between cycles in this while. And you can add anything you wish to these, but everything that you need is in those structs. But it's up to you. And we've written a function that will print out the contents of this giant state struct with all the substrucs that we can write it on the hour grader. Let's look at the individual fields for all of these substrucs. So, so if we have our IFID, that will contain the machine code and a PC plus one. PC plus one is important because you will use that for your BQ calculations in particular. Because BQ always branches to PC plus one plus offset. But really, that's the only reason that we need the PC plus one field. And so we'll use that in the execution stage to calculate our branch target. And then we also have the machine code. And that's helpful for a lot of reasons. You might look at the lecture pipeline and notice that sometimes we only have the the opcode and the destination register. But for the project, it's just going to be easier since it's all one int to just keep the machine code of the instruction and propagate it through all of our different pipeline registers. So that's going to be pretty constant. And our PC plus one will also propagate until we calculate our branch target in the EX stage. After the decode stage, we'll have the values that we read from the registers, separate fields for that. And then we have the offset, which I guess you could just like take out and keep from the instruction machine code, but it's separate, which will help us better represent what's going on with our calculations and additions for LW and SW and BQ. After the execution stage, we'll have whatever you put together from the ALU. And then sometimes we'll also need the value of register B still. We won't need the value of register A anymore because that's only used to calculate the ALU result. But if we're doing like an SW for instruction, for example, we'll take register A plus the offset, but we'll store register B straight into that value memory. So we need to keep register B until we get to the mem stage. And then after the mem stage, we only have what we might potentially write back to a register. And we would also just keep the instruction machine code. Now, I think that's pretty much all the structs that correspond directly to the lecture from pipeline, sorry, the pipeline from our lecture. But there is one that we're going to add, and I'll talk about that in a minute. By the way, we do have this simulator. It's a visual simulator that we've built online. It's available on our website. You can also click the link here. I think it's still available at that link. I may have changed, double-check the website. But this will follow the pipeline from the lecture, will not follow the pipeline from the project. So if you're debugging the project, it'll help you for the most part. But this extra pipeline register that we've added for the project, it will not help you debug that. So you have a little bit of something to help you debug. But be sure to know what I'm about to talk about with this extra pipeline register. Before I get to that, let's do one more thing with the pipeline from lecture. And we're going to go through a program and just visualize what that looks like in a sort of time graph. So just like we had the state diagram for our multi-cycle, this is how we visualize instructions going through our pipeline from lecture. So if we have this program on the right, it's going to have each of the instructions just go through all the different stages. So of course, the first instruction will go through the IF stage first, and the next one, and the next one, and so on, and so on. And one of the things that this means is that at the beginning, not all of our stages will have something. Of course, we have to wait for this first instruction, the LW, to get through the IF stage, and the ID stage before it gets the EX stage. So for a little while, we have just empty space in those stages. And we're going to call those startup cycles. And for the LCTK pipeline, there are four startup cycles. So you can see for these first four cycles going through here, we have nothing in the Ryback stage. So we're not at full efficiency just yet, but because typically our programs are not like seven instructions, like we have here, and there are thousands and thousands of instructions. These startup stages, you're really in the end, don't hurt us in terms of performance. So we're not going to worry about them, but it might sometimes throw off your CPI calculations if you ever need to do that. But after that, we will see that we will commit or write back one instruction every cycle, starting in cycle five. And so even though at this point here, we don't have anything in the IF, technically we do. It's whatever is in machine code after the halt, which might be a zero here, or it might be some piece of data interpreted as an instruction. We know that we won't need to execute that, but the math works out because even though this is in the IF stage, we're not going to call that another startup or wind down cycle because we have them at the beginning. So don't double count your startup or wind down circle cycles. But we can see that instructions just move along as time goes along, and then they leave the pipeline because they're executed. And so that will execute all of the instructions that we need. So now I'm actually going to talk about this extra pipeline registry that we've added for the project. So to recap, here's our lecture pipeline. And our lecture pipeline has this thing called internal hoarding, which is specifically for the register file. So here's a register file. And you can see these purple and red wires that go from the right back stage and they're really kind of snaking behind the processor here, going back to the register file. Internal hoarding allows us to take the LU results or the memory data, whatever we're writing back to a register, and essentially write it back at the beginning of the decode or at the beginning of a right back cycle. So that's the same decode cycle, happening at that same time can read it after it's written back. Now if you have registers that are only controlled by the positive edge of the clock, then that's not really possible. Because that would mean that the register file here would update at the same time as these val A and val B registers, pipeline registers that we have. So if they're updating at the same time, and val A and val B is trying to read from the register file, then val A and val B are going to be potentially behind in terms of what's the most up-to-date data. And so our lecture pipeline adds internal hoarding, which can be implemented in multiple ways. You might make your register file controlled by the negative clock edge. You might add some internal wires and it mucks val A and val B can select directly from the right back stage instead of the register file, which is essentially our data forning anyway that we'll get to later. But our project does not have the internal foring. So only have extra on data foring, no internal data foring in the register file. And that's because if we look back at the general flow of our code here, we will see that the ID stage in your while loop will happen before the right back stage. So any values that the right back stage would write back to the register file will not be read by the decode stage. And we're going to say that that's okay. We're going to add some extra data foring to this project beyond what's in the lecture pipeline. And we're going to do that with an additional pipeline register. So we have this pipeline register here that will just copy the right data from this purple wire. And that will allow you to forward back to the beginning of the EX stage just like you would with any of the other stages. And specifically pipeline registers, not stages. We do get those confused quite a bit. So do not confuse your pipeline registers with your stages. And so to implement this in the project, we've added an extra struct for you. One of those pipeline registers, I was talking about earlier, we've added one for you. And it just again contains the exact same contents as the MemRiteBack pipeline register. It just says the right data and the instruction machine code so that you know how to forward things properly. Now the cool thing is that this doesn't mean that this pipeline, this data path here is some sort of theoretical data path. I mean, I guess they all are. But just because it has internal forwarding, or just because it doesn't, doesn't mean that we couldn't implement this in actual hardware. So this is still a valid pipeline data path that we could build with real components. But just for the purposes of the project, this is what we're going to go with. And I think it's pretty common to use actual internal forwarding in a real processor. We're not going to do this. And it still doesn't mean that it's impossible to make this in physical components. And so I've mentioned data forwarding a lot. And that's going to come from hazards, which we'll get into now. But I do want to cover some other things before we get to the actual implementation of the data forwarding. So we have these things called hazards. And they come from the idea that we are trying to execute five different instructions at the same time. We have, like I said, I'll just write them again, we have our if, our ID, our ex, our mem and our right backstage is. And all of the updates to the register file happen here in the right backstage. But if we have instructions that read data before that data has been written back to the register file, we have some issues with getting old data and everything executes wrong and it just propagates from that. And so we have to pull some tricks in order to make that work out properly. So let's look at a couple examples of the types of issues that we could see with a pipeline. So this first one is called a data hazard and this arrows in the long spot. And the idea comes from what I just said where we might write back something after it was supposed to have been decoded by another instruction in the decode stage. So if you look at these two instructions that I have here, the add and nor, if the add is say in the right backstage, it would update register three with the add in right back. And that means that the nor is in mem. So it's already been through its ALU operation, but it needed the results of register three in order to do that ALU operation. And so it probably had the wrong value for register three, which means that our value for register five will be wrong and it propagates from there. So I'll draw an arrow between the register three here. And that's what we're going to call a data hazard where we don't have the data that we need at the right time. And so that's going to be a lot of what the second half of the project is after the checkpoint, but I'll get to that later. The other type of hazard we might have, at least in the LCTP pipeline, is something called a control hazard. And it's this idea that we specifically for branch instructions might not be supposed to like execute the instructions that come right after the VEQ, because the VEQ will change the PC. And instead of just going to PC plus one, and then we have all these instructions already loaded into the pipeline that we're never supposed to execute. And so if they were to execute, then looking at what it's modifying, it might modify register three to be incorrect, and then register five to be incorrect, and then it might update some value in memory to be incorrect. And we don't want that to happen. It was never supposed to execute, supposed to go somewhere else. And so this is going to be called a control hazard. We have another attract that I, in my opinion, is pretty easy to implement for the project compared to data forting at least. But this will allow us to fix our control hazards. So if you look at the worksheet that's available in the Google Drive, we have this problem posted for you. And so this is some quick practice at finding data and control hazards. And so I think that algorithms pretty, pretty easy. It's our main question for data hazards at least, is is a value that we're reading being read after it's been written back. And then for control hazards, we just look at every single branch that we have. So yes, that's the easy one to start off so if we have a BQ, we know from this slide here that the BQ will be resolved in the memory stage, which means whatever is in the IF ID and EX stages was never supposed to execute. And so we're going to mark the three instructions after the BQ as a possible control hazard. So I like to draw this with an arrow going off to the right that stems from the BQ. Now for data hazards, we have to look back a little bit. We have to figure out just how far a data hazard might mess with us. So if we have a writing instruction in the write back stage, so we're going to put a write instruction. And then we want whatever is reading that same value to be in the decode stage. So that means that in between we have two stages where we don't want anything to read that same value. And so that means that any of the two instructions after an instruction that writes to a register, if they read that same register, then that's going to be a data hazard. So if it's more than two, then it's fine. All right. So let's look here. So starting off with our LW, if we look over, actually technically we should do this in reverse. So let's start with the BQ. If we look at the two instructions preceding the BQ, which is really just one instruction because this is LW, we see that we're reading register five in the BQ. So is register five being written in either the two preceding instructions? The answer is yes, it's being written by this LW. So that means that we have a data hazard there. So now we can move on to this LW, which reads just register zero, which isn't made modified. All right, we're good there. We get to this add, which reads register one and register one. Looking back at the two preceding instructions, is anything or writing to register one? Well, yeah, LWs. So we have the data hazard there. All right, moving on. This add, it reads register one and register five. And if we look back at the last two instructions, only registers one and two have been modified. So we do have a hazard for register one, but not for register five. Moving on to our NOR, let me see that's reading zero on two. Zero hasn't been modified, but two was modified just two instructions ago. So that's a data hazard. And then finally our SW, even though it's an SW instruction, people get confused about this. We are reading both of these registers. And so we have to look at both of them. Well, zero hasn't been modified, but one was modified just two instructions ago. So that's another data hazard. So in total, we have, let's see, one, two, three, four, five, six data hazards and one control hazard. And I guess you might technically count the double read of the add one one as one data hazard. I'll call it two. And so now we have to figure out how to fix these data hazards. And the first thing that we can do is this technique called avoidance, which just allows us to insert a bunch of no ops. And remember that no ops, no operation won't change the behavior of any code. And so we can put them into our code to separate out all these instructions with hazards and then eliminate the hazards. So let's look at an example and then we'll talk about the pros and cons. Right. So looking at the same code that we had, part B of this problem, we will insert a bunch of no ops to fix those hazards that we had before. So I think it will be easiest to start off with the control hazard. We know that, like I said before, the BQ will resolve, meaning pick whether or not it's supposed to take the branch or not in the mem stage. And so we need nothing to be in the the IF ID or EX stages if we take that branch. And so we're just going to put three no ops right after the BQ such that whenever the BQ reaches the mem stage, no matter if the branch is taken or not, we're safe because we have nothing in those other three stages. And so then once the BQ finishes, whatever next instruction comes along, I will come along and execute, which in this case would be the LW. All right. Now let's look at our other data hazards. So starting with the BQ, which I said had a data hazard with LW above, we said that we want to space out the BQ and the LW or go back to my picture here. So if our writing instruction is the LW, our reading instruction is BQ, then we want to space them out enough so that nothing is happening between the two instructions while the LW is in right back in the BQ's Indicode. And so the easy thing that we'll do will just put no ops into those two stages. So let's say let's add those junops. So we put two no ops between the LW and the BQ. All right. Let's keep going. So we said our next data hazard came from this ad being dependent on the LW. So the same thing, we want LW to be in the right back stage at the time the ad is in decode. So we'll need two ops in between two no ops in between to make our EX and our MMP while those instructions are writing and reading data. So put two no ops there. And the cool thing is that this fixes the other data hazard from the second ad. So we said that the second ad was also dependent on the LW. But now it's been spaced out even more. So if we take this ad and we look back at the last two instructions, we see an ad in no op, not an ad in LW anymore. So we've eliminated that data hazard also. Now let's move on to the next one. We said that this nor was dependent on the first ad. And so let me draw all the little box again. Let me draw it up here. So looking at it right now, so we would have our ad in right back. And then it looks like we would have the second ad in probably minimum. It could be an execute, it didn't really matter. And then we wanted that to happen at the same time as our nor is in decode. So now we need just one no op to put into the X. We don't need to put into. So I'll just add one no op here instead of two this time. And finally, if we look, we said that the last SW was dependent on the second ad, but now they're spaced out again. So at the time that the SW is in decode, well, that means that the nor will be in EX, the no op will be in MEM, the ad will be in right back. And so this no op here, single no op that we've inserted has also taken care of this last data hazard. So again, just to recap, SW after we've inserted the no op, we look at the previous two instructions, we see a no op that don't modify version one or zero. And so we've eliminated that data hazard. So here's the actual, just written out solution in case you wanted to see it. And one thing to note is that this does work for the lecture pipeline. We haven't gotten to the project pipeline yet. So don't get ahead. So now I will talk about what's actually happening in the project. So the issue with this strategy called avoidance is that it pretty much bricks all of your project one S test cases, including molds, including your combination function, all that machine code just doesn't run on the project three pipeline. If we have to insert no op for every single data hazard, I mean, in mind, if you just happen to have zero data hazards in your code, but that's pretty rare because we like to build upon values with different instructions all the time. So basically bricks all of our old programs, and that's pretty bad. Also, notice how we like doubled the size of our code here. Like we had seven instructions before and we inserted more than seven no op looks like it looks like eight no op's. So we've more than doubled the size of our code, which we don't want. And so even though it will work, it'll work, it'll produce the same program behavior by inserting these no op's. We've forced the programmers, the simply programmers to insert those nops and it takes a lot of memory. So we're going to look for other solutions, but for the project checkpoint, this will be good enough. So the project checkpoint is going to use avoidance. The project checkpoint will take all the test cases from the final project three alter grader and it will insert a bunch of no op's to make sure that a pipeline that requires avoidance would work with these test cases. So you don't have to worry about any other hazards, really any hazards at all for the project checkpoint. You just need to worry about getting each stage to work for each specific instruction for the project checkpoint. And so that's why it's not great. It is because it's not full functionality, but that will be the project checkpoint. So the hope is that by submitting to the project checkpoint, you do know that your code is working for each stage for each instruction. And then in the last week of the project, you can you can fix all the forwarding and hazard related things that will be in the final submission for the project. Now I did mention a couple of minutes ago that the project pipeline has one more pipeline register, which you can think of as one more stage. And so that's going to mean that this little strategy where we just inserted two or three no op's or whatever isn't going to work the exact same way. Now we want our hazardous instructions to be spaced out one more. So here's why. So looking back at this code, let's take the same example of the LW and the BEQ have out. So if we have the BEQ in the decode stage, before we could put the LW in the right back stage, and then as long as those things are happening at the same time, we're good. But now, because things are going to be read from the decode stage, before they're written back in the right back stage, we want the LW to be out of the right back stage. So we want it to be done. We want it to be in the pretty much the end spot. That should not be a right back. That should be an LW. And so that means that we do have to space it up by one more. So before we didn't, we could just allow the LW to be in the right back stage, but now it needs to be out of the pipeline before that happens. And so now we need three nops before we fill the EX and the men stages with nops. But now we will also fill the right back stage with a noop. And so that's going to mean that we're going to add this extra knob here. This doesn't affect control hazards. So those will stay at three. We'll add this knob here. Add this knob here. And so now we're going to space outer data hazards with three nops instead of just two to allow everything to be properly updated before the next instruction exks. So the project checkpoint is going to use this algorithm where we add three instead of two. You can trust that that's happening with the test cases on the project autograder. All right. If you want to make sort of your own checkpoint, when you start implementing data forwarding, which I'll get to a minute, you can take out most of these nops. So for example, you might keep just one. Every time you have an LW followed by dependent instruction. And that's because if we look at, well, let's see, do I have a slide for it? I'll get to that in a little bit. But that might be useful to you. So you might implement your data forwarding for most instructions, but keep out the inserted nops for LW and keep them out for the BQ. So you could generate some code like this on the right that only tests your normal data forwarding and not your other hazard resolution schema, which we'll get to now. So we said that adding all these nops was pretty detrimental because it increased the memory size of our program pretty significantly. And we want to avoid that if we can. So we have this strategy called detect install, where we'll just do it in hardware instead. So we can use pretty much the same algorithm. We can look back at the previous three or two instructions and see if the register that's written to is going to be read by the instruction currently in the decode stage. And if it is, then we need to wait for that value to be written back first. So we'll keep whatever instruction is currently in the decode stage. We'll keep it there in the decode stage, but we'll just send a no up to the execute stage. And so this is called detect install because we're stalling whatever instruction is in the decode stage and just sending no up to the rest of the pipeline. And so we won't use this in the project generally of the one exception. So we write this. So the project will only use this for an LW, immediately followed by a dependent. By dependent, when I say dependent on me, an instruction that reads whatever value that LW is writing to a register. But otherwise we won't use this in the project. So for example, we have if we have an add instruction, we had our classic example somewhere of an add writing to register three and then immediately a nor reading register three. So a detect install, normally, we might just stall the nor in the decode stage for I think three cycles and send no ups to the execute in the memory until add writes back. But for the project, we will use a different trick to fix our data hazard. We won't have to stall nor. But for an LW, whatever instruction comes after an LW, if it reads that same register, the LW writes, we will have to stall it for exactly one cycle. We'll see what I'm in a minute. And so some things that detect install does for us. It's going to allow all of our project one test cases, assuming they don't include Jailer, at least on LCTK pipeline. They allow them to run correctly. So you can bring in your molt files, a test case for project three at that point. And we don't have a giant memory footprint anymore. The thing is because we're inserting the no ups in the same place, just now in hardware, our performance, the number of cycles we execute the entire program is going to be the same. So it hasn't really improved our efficiency beyond memory. So 281 might be a little happy. That we've reduced our memory footprint, but we haven't really reduced our runtime yet. So we're not completely happy yet. So before we get into our final track, I do want to show you what this looks like. So if we have this program, which has a lot of data hazards, I think it's the exact same program that we just did. So looking at the hazards, let me just remind you where they are. So we have a control hazard here. We have a data hazard here. We have a data hazard here and here and here, which is all the same one. We have a data hazard here. And then it has a here. Let's see how we would implement that in hardware with detected install. So the LW will just go along, all five stages as normal. And then the BQ will come along. It'll see that it's reading a register that was written by a previous instruction. So it's going to wait until that instruction is in the right back stage in order to do its decode. So that means we have to stall the BQ for two cycles, at least in the electrified one. And then it'll move on doing it's whatever it wants to do. But because it's a BQ, we have to wait until the memory stage to know if we're supposed to actually fetch the next instruction. So we're going to stall the fetch stage until the BQ is done, updating PC to actually fetch the next instruction. It means that IF for the next instruction will occur at the same time as the right back for the BQ. And so ultimately it looks like we've stalled five cycles here. But these two first stalls only exist because the BQ was stalled by two. And then these three stalls are actually because the BQ. So moving along things are going to look similar. So here we have an ad that that depends upon the LW. So we'll wait until the ID and right back stages match up, which will of course stall the next instruction again. These stalls are all for the same reason. And then we'll have one more with the NOR. We're waiting for this right back to line up with this decode. And the same for the SW or waiting for this decode to line up with this right back. And that's going to fix all of our hazards for us. So the final trick that we have is called detected forward. And if we look at our pipeline diagram, we have all these pipeline registers, which will hold these intermediate values for us as they propagate through the pipeline. We figure that we can just take those and draw some wires to go back to the AU mox's and send them to whatever calculation the next instruction is going to make. And we can do this because for most instructions, particularly the ad in the NOR, whatever calculation is going to be made is available at this point. So at the end of the execution stage, you generally know what you're going to write back to the register file. Whether that's an addition result or a NOR result, whatever you're going to write to a register, you have it at that point. So you don't have to like really wait for it to write back. If you just write a wire that goes from that point, really this point at the end of the pipeline register, back to the mox for execution for the next instruction, then you're still getting the correct values. The one exception is for LW, because for an LW, we don't know what we're going to write back to a register until the end of the mem stage. So we actually have to like get the thing from memory. And so that's why we have to insert an extra NOP with detect install for an LW followed by a dependent instruction, because we're always going to be forwarding to the beginning of the execution stage. So all these wires that's kind of messing in the mox is we always forward to here. So write that down, forward to here, and we might forward from here or properly from here right after the pipeline register. And of course, for the project, we will have one extra pipeline register that we'll have to forward from. So it might come from here also. And that's data forwarding. So the idea once again is that we don't technically have to wait until the write back stage to actually get to the value that we need for the next cycle, the next instruction. The only exception is for LW, we want to put some sort of NOP in the execution stage so that the LW can finish the mem stage before the next instruction uses that value. So that's data forwarding. And then finally, we do have this one thing called speculating squash, which is for control hazards, where when we mispredict a branch, which in this class is going to mean when we figure that out that we're supposed to take a branch and not just go to PC plus one, we'll just overwrite all instructions in the previous three stages. That is the execution decode and fetch stage, which is overwrite them with NOP. So they do nothing and we can branch and get the same behavior. So now let's visualize what that looks like. So looking at our code, the data forwarding is going to eliminate most data hazards. The only exceptions are going to come from an LW followed by a dependent instruction, which will happen here and here. And let's assume that we're not going to take the branch here because negative one is not equal to zero, so we won't take that branch. And so now we've eliminated most of the stalls. And the only stalls that we have now come for the LW followed by the BQ and the LW followed by the add. So at this point, we want the execution stage to come after the mem stage. So the execution of the reading instruction will come after the memory stage, the LW. So they'll happen there and that'll happen here also. And just to look at how our other hazards have been handled, we want to the execution stage of the dependent instruction will come right after the execution stage of the original instruction, I guess, the writing instruction. And I don't think we have any actual examples of that in this code here. But if we, I don't know, maybe change this in order to read register one instead, then we would see the execution before it is from here to here. That's what that would look like. So as you can see with this data forwarding and the speculative execution, meaning that we're allowing these three instructions to speculatively go into the pipeline on the assumption that we might execute them and allow them to go. And if we don't, then we'll just put it up there. We've shaved off a total of six cycles, which is pretty good because we have seven instructions. So we've increased our CPI or we've decreased our CPI tremendously, which is a good thing. And so that's what you'll be doing in the project. The only difference is that you will be forwarding from this additional right back end pipeline register. But the cool thing is that the actual execution and the number of stalls you have to insert will be the exact same as the lecture pipeline. Because in our final project three, we're not doing detect install or except for that more now that we're doing detect and forward for all the rest. And so some final project chip tips and notes before I go here, we aren't doing jay learn project three. So that means don't submit to the auditorator any tests contain a jay learn won't work. The flag your submission is wrong. So don't do it. Same as project one asks to make sure that's in any variables are initialized properly, particularly to zero. And for the checkpoint, you don't have to really do anything for data hazards. But for the final submission, you will be using detect and forward. If you want to go ahead and try to implement detect and forward and check that it doesn't break the rest of your code, you should still be able to submit that to the checkpoint and get full test case pass counts on the checkpoint. So don't be afraid to do more, but this is really just to get you working early on it. Right. And then the spec has other things like for branches. And I'll talk more about this next week. Why this matters in general, but for branch predictors, we're going to speculatively predict that branches are not taken. And then when we find out we're wrong, that's been we'll go squash. Right. Yeah, you can see the spec for a bunch of other details. I always encourage people to post on piasa with test case ideas. I like to just write a test or not a test a list of things that I want to test for on every project. So I recommend that you do the same. I just don't include jaylor and try to include as many possibilities for data hazards in your test case ideas as possible. Every maybe you know every instruction write that with a data hazard, just to test that it's working properly for every instruction. Right. Because like I said, the bugs, the bugging structure solutions are the hardest to catch for project three. So you want a really good test suite. Right. And this slide, which you can look at, even though the bugging structure solutions are really hard to catch. And generally that means that your test suite is good, just because you get all of them doesn't mean that it's a perfect test suite. So make sure to cover things that are exposing your own solution also. That would be really good. And building tests from small ones to large ones, like including your multi as perhaps is a really good idea. All right. And that's everything that I have today. Good luck on the project and homework and all those things. And we'll see you next week to talk about efficiency and how we can improve that on all the data paths we looked at so far this time. All right. See you next week. Everybody.