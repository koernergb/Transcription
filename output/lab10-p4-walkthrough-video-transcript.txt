 Welcome back to each 370. In this video, we're going to be going through the Project4Spec example. And so I'm going to assume before we start that you've already watched lectures through the lecture on direct map caches, you've watched all the labs through the cache lab, and then you've also read over the Project4Spec. So let's go ahead and get started. So before we get into the actual example, I do want to point out some of the features of the starter code. The starter code is very open to change. You have a lot of design choices because the output doesn't directly translate to the starter structs that we give you, unlike other projects such as Project1S and Project3, where the states that would give you correspond directly to what the output looks like. In this project, we're not going to actually check your cache contents, or we're just going to check the messages patched in between the cache and the processor and memory. And so there are multiple ways to configure your cache, but we'll go through what we've provided so far. So speaking of the messages that we'll track, all the output in this project is going to be based on what we call these action, these print actions. And so you have five possible print actions for a right back, no allocate, or sorry, right back allocate on store cache. And so we'll have a cache processor that is where you get a hit of some sort in the cache and you transfer the data to the processor upon a load. We have a processor to cache that is where you find the data in the cache and you write to it. So it only happened for store instructions. So I guess to recap the cache to processor that is only for loads and instruction fetches. Then we have the processor to cache, which is only for store instructions. And then the other three could happen as a result of any access to the cache. So if we are going from memory to cache, that means we had some sort of miss and we need to transfer what is not in the cache to the cache. So that could happen on load and it could also happen on store because we have an allocate on right type cache could also happen on fetch. We also have cache to memory that is where we are evicting a dirty block. And so that could happen again on load fetch or store, but it has happened for a block that has some sort of previous store. So if a block has not had a store since it was last barbed into the cache, then it will be clean. And we will use the other prediction on eviction, which would be a cache to nowhere. So these last three can occur anywhere, but under certain conditions. And so these will only happen on misses. The first two will happen on both hits and misses. All right. By the way, do not touch these, I would recommend. Do not also touch the print action function since that works. As we have designed it, really most of the design choice that you have is within the statistics function and within the initialization of the cache, as well as the actual cache struct itself. And let's move on. This is the block struct that we have provided for you. And so it has really two main parts. It has the data, which we might remember from the lab is the actual stuff that is from memory. And then we have all these other things that are really overhead. Technically, the set isn't stored in a real hardware cache. So that might be considered some amount of extra. But all the other things are overhead that has to be present in a right back cache. So that would include the dirty bit, the valid bit, the LRU label, which you have a lot of leeway on to decide what exactly that means, and then the tag. And if you feel like you need to add anything else, you can definitely do that. But all this down below the data would be considered overhead. And then the cache struct itself is made up of a whole bunch of those blocks. So if you partition out your blocks, you can come up with any sort of cache configuration. So for example, if I have an array of just four blocks in a fully associative cache, I could use all of them to belong to the same set. Any direct map cache, I could have each one be its own set. For some set of set associative cache, I could divide them up into different chunks to belong to different sets. That is one way to do it. You can also use the explicit set variable that we have provided in the block struct. That's up to you. But in real hardware, we would probably do the partitioning scheme, except it would be more automatic. All right. This depicts how your code is going to flow. And this project is unique in that we are working on really two separate components. And one of them is already finished for us. That is our simulated.c file. This is from Project 1S. So you have a couple options with this project. You can use your own and make certain modifications. And so check the spec. But some of those modifications would be to call the cache in function once at the beginning of your program. And then to call the print sets function once at the end of your program. And then all of your memory accesses, so anything referencing state.mem within your main Project 1S loop and not in the initialization, you'll replace that with a cache access. And then the cache access will do the things to see if it's in the cache and if necessary make a couple of memory accesses. And those memory accesses will be able to come back to Project 1S and access state.mem. So the cache won't directly access state.mem. Your main loop in your Project 1S file will not access state.mem. It really only the initialization where we read in the machine code file, set all the other addresses to zero. And then this this mim access function. Those will be the only two parts that actually touch state.mem. So just to recap, cache in it will be called once at the beginning. Print stats will also be called once at the end. And then cache access will be called many times depending on what the machine code is time the processor to do. And so at least at every fetch and load and store. And so those will become the three sources of our address references. Now one hint, make sure to treat instruction fetches from the perspective of your cache the exact same as a load. Even though they're going to appear in two different places within your main while loop, you can treat them the exact same. It's only stores that you will treat slightly differently. All right. Let us move on now. One other thing before I move on, you can also use the instructor provided, approach one s simulator. We provide it as an object file. And if you remember from project to L, an object file is compiled a similar code or compiled machine code. And so you won't be able to access the C file if you are going to use the instructor version. But because of linking, we can take some cache C file, compile it to an object file, and then link it with another object file to get our final executable. And that will be your actual cache simulator. And so we do this because it speeds up compilation. We only have to compile the simulator once into an object file. And then from there every time we make an edit to the cache, we don't have to recompile the whole thing. And also we do not want to freely get out the instructor solution to project one s. But you are still free to use yours. And it would look a little something like this. The optional part would add in your C file up here, which you can choose to compile once or compile over and over again, depending on how well it is working. And so you can trust that ours is working even if yours isn't. So if you're having struggles trying to figure out how things go, you can use ours. But it might be trickier to debug. If you want to debug, you won't be able to look inside the simulator, but you can set breakpoints and cache init and cache access and print stats to see what is going on and run your debugger as normal. So let's actually get into the spec example. Now all of the tests for project four will be named in a particular scheme. And the name will specify what our cache setup is going to be because different assembly code files might produce different cache behavior, depending on the actual parameters of the cache. So whenever you make a test name, you'll have some name and then dot and then you'll have a block size in words. So that is going to be for for the spec example. And then you'll have a number of sets. So that's going to be two here. And then you'll have a blocks per set. And then you'll have your regular extension, as or S or LC to K. And so for the spec example, we're going to work with a block says a four, two sets and one block per set and one thing to note. If you ever see one block per set, that means that we're working with a direct mapped cache on the website. If you ever see only one set, that means that we're working with a fully associated cache. And so I guess there is a possibility that you could have both a direct map and a fully associated cache. Really, in that case, would just be kind of like a temporary variable or buffer instead. With any other parameters, though, that means that we're going to be working with a set associated cache in order to understand those and make sure to watch the set associated cache lecture. So this is a direct map cache. And since it's direct mapped, we can ignore LRU. However, if you write your LRU to work for set associated caches, then it'll work the exact same for direct map caches and a fully associated caches. So one of the keyestest project is to generalize the set associated because a fully associated and a direct map cache are each types of set associated caches. And so this image here represents what our overhead will be in total. So we can map this blue section to the data portion here. And then we can map all this overhead to the other things that we see in this image. About a bit, the dirty bit, the LRU, which isn't using the direct map, and then the tag. So now let's look at the actual code. Let me just disappear. So the assembly code program that we've given you for this example is very simple. It's only four instructions. And I want to point out a couple peculiarities with it first. The first is that we are accessing addresses that aren't necessarily in our machine code range. So for example, the very first instruction access is address six because we take register a, which is zero plus constant six and we get address six. So the very first element that will be modifying is right here. And we'll start off as zero. And in Project 1S, we wouldn't see that in our print state. However, we have modified the, the MIM access function for this project. So that'll make sure that I'll print out anything that is written to. So that means that anything it doesn't print, you can assume is still just zero within the memory. Now, depending on your stores, I might still have some other value in the cache. But it would be zero in memory until it is printed out. And so we'll see that as we go through the output in the end. Same goes for address 23 and 30. Those will start off as zero. It was actually remain zero because we're only loading from them and not storing to them. And so they might not appear in our print state, but we can trust that they are in memory and they start off as zero as is our convention in LCTK. So the question that I think trips up most students who are storing this project is how our cache is laid out with a particular aspect. And one of the things to know about our cache is that like memory in LCTK, which is Von Neumann, it will hold both instructions and data. So that means that when we are first running any program, our first address access will always be to zero because our PC starts at zero. And we would always in our project 1s simulator, accessstate.mem at PC. So now instead of accessing state.mem directly, we'll go through the cache. And the cache will always find that is a miss on the very first cache access. So we will also bring in PC zero from memory. And so we'll see this kind of alternating behavior. We'll do an instruction fetch and then we might do a load in the store. We'll do another instruction fetch, maybe another load in store, and we'll go back and forth and back and forth. And we will see more cache accesses from fetches than anything else. So let's look at the simulation for that first access. So first access is to address zero. And that is to fetch the actual machine code. And so we're going to give this machine code to the processor, to our project 1s portion. So the processor knows what exactly to execute. And so that will include the machine code for the SW. It'll see that it's an SW. It'll add a REST or a plus offset, but it can't do that until we give it the correct machine code. So if we were to just give it a zero, it'd be incorrect. And so whenever you see a return in your cache access, make sure that you're returning actual data. Because if we just give the processor something that is invalid, then it will not produce the correct behavior. So we know that our cache has two sets. We know that the block size is four words. And so on this very first access to address zero, we are going to access the entire block that contains zero. And because the block size is four and zero itself is divisible by four, we can start at zero and count one, two, three, four words. And then all those will be loaded as a block into our cache over here. And so that means that we will change our valid bit to a one. This is only a fetch. It's not a store. So we'll keep our dirty bit at zero. And then we will get the appropriate tag. Now remember that all blocks or sorry, all addresses within the same block, share a tag and a set index. They only differ in the block offset. And so we can see this and the different columns of this table over here. They all have tags of zero. They all have set indexes of zero, but the block offsets range from zero to three. And so we will move them all around as a unit. We will update our tag in the cache. And then we will be able to move on after returning the actual machine code for the store instruction. And so from that, we would get a couple actions. First, we would print out that we are transferring the entire block that contains this machine code from memory to the cache. So that is from addresses zero to three, because that makes up this very first block. And then we need to actually get the machine code to the processor. And so we will transfer that from the cache. One thing to note, we're not going to transfer all of what we just brought in in the block to the cache. So we're going to transfer a block, a big chunk from memory to cache. And then we will transfer a word, a much smaller chunk, to the actual processor. We don't need to transfer zero to three from the cache to processor. And so we will see only zero to three go from memory to cache and zero to zero go from the cache to processor. That brings up a generalization that you can follow with this project. Whenever you see a cache to processor or a processor to cache, you should see only one word being transferred. But when you see something from memory to cache or cache to memory or cache to nowhere, you should see an entire block size load. So since our block size is four, we see four addresses in the range here. And then here when we're going to the processor, we just have one word that is being transferred. So now our processor will actually interpret the instruction. We will address your A, which is zero plus six. So we know that we will next be storing, register one, which will start off with the value zero. We will be storing that to address six. So when we get to that in our cache, we will first look through our existing cache and we need to figure out if that address is present in the cache. So looking through it, we know that if we were to take six and six in binaries, a bunch of zeros, and then we have one one zero. So our last two bits would be our block offset. We know that because the log base two of four is two. And then we would look at one bit of set index. And we know that because the log base two of two is one. And so this here is our number of sets. And the four here is our block size. So we can see that we're looking for tag zero within set number one. And set number one right now is invalid. So we have nothing in there. We will get a miss. And because we are doing an allocate on right policy, we will need to bring the block containing address six into the cache first. That will be our very first thing that we do. And so that will include addresses four through seven. We know that because all those, once again, share the same tag, which is zero, and the same set index, which is one. So the quick and easy way I like to do it, I know my block size is four. So I go down in my address from six until I find an address that is a multiple of four. So if I go down from six, five, four, I see that four is a multiple of four. And then I will start there to bring in my four words that will define this block. And so they'll all start off as zeros. So we'll bring in these four zeros into the cache. And then we will overwrite address six, which is block offset two with the new data, which also happens to be a zero. So we'll bring in zeros overwrite one of them with another zero. And then we have to mark this block as dirty because we have now scored to it. Now in theory, you could make some optimization where you don't mark it dirty, because you're not actually changing the data. We will not be doing that for this project. We're just going to see if it's a store and mark that. There are various reasons for doing that. Oftentimes, if we try to put that computation in the cache itself, we get more complicated logic. And our access sign for the cache becomes longer. So that is not something that is typically done out in the real world. And so we won't be doing it for this project either. But now this block is dirty and has a tag as zero inside of set one. And so this will produce these two productions. Because our set one was empty, we didn't have to evict anything whenever we got that miss. And so then we brought in the words 4 through 7 from memory to the cache. And so once again, we know that that 4 through 7 that has a range of size block size. And then we transferred 6 6, which is just one word from the processor to the cache. So before we had cache processor with word zero, and now it's just word six from processor to the cache, because we're doing a store. Let's move on to the next instruction. Next, our PC will increment to one. So our PC is now one. And then we have to fetch the machine code for the instruction at PC one. Now breaking out the address, we can see that this binary address here is address one. Our block offset will be one. Our set index will be zero. Our tag will be zero. And so if we look into our cache, we find a tag zero within set zero. And we see that it's valid. So that means that we will have a hit for the first time in this example. And so we can immediately index into the state of block with our block offset and return that to the to the project one as portion. And so we don't have anything to transfer from or to memory or out of the cache. So we will just transfer this single word from the cache to processor. And then we will decode it and continue to execute it. Now this LW will be accessing address 23. There's note that our registers zero and one still contain value zero. And we are loading register a plus offset. So zero plus 23 is zero. And so we will be loading that into register one. Now conceptually, we know that address 23 still has a zero. So the actual contents of register one won't change. We'll still load that into the register. So let's look. First we have to see if 23 exists in our cache. So 23 in binary, let's see that's 16 plus four plus two plus one. So I get a one zero one one one. So that means that my tag right here becomes two. And here are all the leading zeros for it. Our set will be one and our block offset will be three. So if I look in set one, I see that my tag of zero that currently exists in the cache does not match my tag of two that I'm looking for. So I'm going to get a miss. And so that means that I have to make room for this block that contains 23. So first we are going to be evicting this block in set one that has a tag of zero. And we will remember from before that at least contain address six. I'll actually remember that that contained addresses four through seven. So that's the block that we will be evicting here. Now because it is dirty, we will have to write back each of these blocks, or sorry, each of these words in this block back to memory. And so that is because our dirty bit is allocated on a block size granularity. We're not tracking dirtyness per word within a block. We are just tracking if the entire block is dirty. So that means that we will write back the entire block to the cache. So our next print action, let's get ahead for a second, will be transferring four through seven from the cache to the memory. So this is a dirty eviction. So now it's out and we need to bring the block containing 23. Once again, we can work down from 23 and to our next multiple of four. Now be 20. So that's where we will start from. So we will bring 20 through 23 into the cache. And of course, these are still all zeros. So our actual cache data would have all zeros for now. That might not always be the case, especially if you if you've written something, or you're actually accessing something in the data section of your program with the dot fills. But for now, they are all zeros. So we will bring that block from memory to the cache. And then finally, we will actually transfer the fetched word, which is number 23 from the cache to the processor. Now our output looks like this. Let us move on. Now our PC will once again increment to two. PC is now two. And we remember that our set zero still has addresses zero through three. So we will actually get a hit. And so that will be pretty simple, but it's the same thing as before. We check the valid bit and set zero. We check the tag to make sure it matches our expected tag of zero. And then we can index directly into that and return the machine code for LW zero one 30. And now we will decode the LW zero one 30 and execute it and then access memory. So our address will once again be 30. And once again, we have to break that down into the tag set and block offset. So 30 in binary is 16. Plus eight plus four plus two. And then I guess plus zero. So if we break it up, we will have a tag of three, a set of one once again, and then a block offset of two. So we'll go into set one, which is just this block. We will check the valid bit and the tag to make sure that the valid is a one in the tag matches. In this case, it doesn't. So we're going to get another miss, which means that we need to evict the existing block in set one. Remember that that contained 20 through 23. So we will have to evict all of those now because our dirty bit is zero. This is not a dirty block. And so we don't have to write back to memory. We're just going to kick it out. And the reason we can do that is because the cash state of this block and the memory state are exactly identical. We know that for certain. And so we can just throw away the data from the cash because we know that a backup is in memory. So we'll evict it and bring in the block containing 30. Once again, we can work down to our next power of multiple of four, which would be 28. So we will low 28 through 31 to define this block. And so we will once again to the eviction bring in the new data. I'll all start as zeros because we haven't modified anything in that block. And it's not in our initial dot fill section. And so then our output will look like this will have this cash to nowhere. That's the throwaway part. And then we'll have a memory to cash once again. And then we will transfer word 30 from the cash to the processor to fill the load and place that value into register one. Is that is a register being in the LW. So now just to recap our cash looks like this, we still have the machine code for addresses zero through three and set zero. And then we have addresses 28 through 31 in our other set. So now let's move on to the last instruction. This is a halt. Our PC will increment to three before we fetch the halt. And so that halt is that address three. The same deal as before. We can break it down into the tag set and block offset. We will remember that that is zero zero and three. And so we will check set zero is that is our set index. We'll check the valid bit. And it is a one. And then we'll check the tag to match. And it does match is equal to the one we're looking for which is zero. So we can index directly into it. We've gotten a hit and we can return the machine code for the halt to the processor. And once the processor receives that, it will be done. And so our program is over. So now just a couple of cleanup things. We will have all of our print actions printed out. And I actually don't have the rest of the output in these slides that needs to be printed. But we will also print the final state. And the number of memory accesses. I'll show you real quick how to calculate those. So first, let's just go over all of our print actions. Once again, so we had a couple of memory to cash. So that would be here, here, here, and here. So that's a total of four memory to cash actions. We had just one cash to memory action. That's because we had one store. And we evicted the block with that one store. And we had to pay for that. And the other thing else was either cash to processor or processor to cash. So I've highlighted in red the print actions that resulted from our machine code. Our fetches that is. So we only had four words machine code. So they all fit in a block. And we were able to keep them in the cash the entire time. We never had to evict any of our instructions. We just had that initial memory to cash. And then we had four subsequent cash to processor actions. And all the others in white were actually handling the loads and stores from our program. So this first section here was transferring or was working to get address six. And then we had the actions for address 23. And we had the actions for address 30. And we had the data accesses in this particular program. And it ended up being a lot more costly in terms of things for the cash to do. And now, like I said, we will print out the final state. The final state is the same as project 1s with one exception. This will contain the actual contents of DRAM. Up until either the highest address that was present in our. And then we had the actual contents of the machine code. Our input to the simulator. Or the highest address that we wrote to. So the highest address that we wrote to whenever we were evicting that dirty block from the cash. Would have been address seven. So our final state would print out memory through seven. And because we were only working with zeros after these. The machine code for these four instructions, we should see just all zeros. So we haven't actually really changed anything with this project. Also note the only register we modified was register one. And by the end, we still loaded the value zero into register one. So at the end of this particular program, all the registers would contain. Just zero. But our PC would be four. Is that is one after the PC for the whole construction. So PC's go zero, one, two, three. And then that would be our final state. So our final state for this, we would have registers. All zeros are memory. We'd have our machine code. Then zeros. And for a PC, we would have four. After the final state, we will also print out the number of memory accesses made. And so all this means is the number of times that you should call the men access function. Which we will remember is available to the cache, even though it sits in the simulator portion. So it is a global function over in the simulator part. And so it can be called by the cache part, even though it is compiled in the object file. And so every time we get a cache miss, we should be loading in all of the words for the block that we missed on through this memory access function. So at least for the four protections that I've dotted here, we should call mem access four times four times because that's the block size. Also whenever we are writing to memory, that is when we are evicting a dirty block, we should be calling memory access for each of those words that we write back. And so we would call it four more times there. We would not call it for cash to nowhere because we shouldn't actually be accessing memory whenever we do a cash to know it is that's inefficient and it curves unnecessary rights to the memory. And so in total, we see five print actions that interacted with memory, each of those will call mem access for every word in the block. And so we will get a total of 20 that is five, your number times your block size, which is for memory accesses. So in that case, it'd be 20. And that is everything I have for for this project example, good luck on the project and make sure to post some he has a if you have any questions.