 All right, hello everyone. Welcome back to each 370 discussion. Today we'll be looking at caches, which is the topic of project 4. And so we'll get more of a conscious behind project 4, what things you might be thinking about when you're debugging project 4. We won't really look at any code or any data structures or much of that, but we will go through the concepts necessary to understand project 4. So before we do that, I do want to show the project 3, score distribution, a lot of, a lot of these submissions in the last day or so, the project not doing so hot. So definitely for project 4, it's the last project finish off strong start early. And we will hope to see a score distribution more like this one from the fall, where there weren't so many scores from that last day that were in a low range. Right, so to talk about caches, we'll talk a little bit about the motivation behind them just to review from like, you're just a little bit. And then we'll go into more of the implementation stuff. And so we've been looking at data paths, we started that before the midterm, the single cycle and multi cycle. Now we've gone through pipelining and we've been looking at ways to reduce our CPI and all that. But at a certain point, we see that the memory component is going to drive our clock period. And there's not much else that we can do to improve our performance if our clock period is tied to memory, because compared to everything else, memories going to be pretty slow because we figured out how to optimize other things like the ALU and the rest of our file and all those. So we need a new approach. And the idea is to only store a subset of what's in our memory, actually inside a processor. And of course, if we remember our memory hierarchy, anything that's smaller is going to be faster. So if we just have this subset called the cache inside a processor, it's much faster than we can get a clock period down and truly take a vantage of all the other CPI improvements that we've made so far. The only downside is that if we ever need something that's not in the small subset memory, we're going to have to solve a bunch of cycles and go get it from what we will call main memory. And that will increase our CPI. So in the end, our CPI will increase, but our clock period will go down tremendously. I hope is that in the end, our actual performance improves pretty well. So our time per instruction overall should improve if we're doing things right. And so we can apply this concept not only to memory, but we also think of hard drives. Memory is a lot of times going to contain things from a hard drive. Because a hard drive is a pretty slow thing, even with solid states nowadays, it's still pretty slow. So memory is going to be a subset of that. And we still have to figure out how to get things from the hard drive to memory and how to use them efficiently. Because of course, if we had our hard drive directly connected to a processor, our clock period would be like on the level of like milliseconds instead of nanoscience, which would not be good. We can think of this in terms of a couple different analogies. My favorite one is the library analogy. You can think of, well, I mean, say you're doing a paper, you're doing some homework, and you need all this data in order to construct whatever it is you're doing on the homework and paper, your argument. So you can think of registers as perhaps the one book you have open on your desk or maybe the actual piece of paper that you're writing on. Those would be your registers. The things right in front of you that you're working on. You can think of the cache then as sort of other books just kind of stacked up on the table. They're not really open, but they're just kind of stacked there. You'll figure that you'll use them pretty quickly. And then maybe you think of main memory as perhaps the section of the library that you're in. Things that you're kind of close to that you could go get pretty easy. It would take standing up and going and getting it, but you could go get it. And then you can think of the hard disk as the entire library. If we take a one step further, you can think of perhaps the internet or network as the networks of libraries. I don't know if you've ever like gone to your local library, the UN Library and like asked them to order a book from somewhere else. That would be like a network of libraries, which has latency on the level of days, just like networks on computers have latency on the order of seconds or so. So if we think about this, we can think of the different hierarchies. It's relating size and speed. Because of course you don't want to pile tons and tons and tons of books on your desk. Also, never be able to find one that you need and you'll have all your space clogged up from the actual thing you're working on. So we're going to order what's close to us by size. So things that are closer, or not going to be having that much clutter, but things that are far away can be practically injured in size. And so that's kind of the idea. So that cache is the stuff between memory and your registers that's going to be sort of fast, but not as fast as registers. Definitely faster than memory and not containing as much as memory, but definitely continue more than your registers. That's kind of the idea here. You might notice that you use this in everyday life. So if you have a phone like mine, you might have like this bar at the top of your apps list. That's like the most recently used apps. And if we think of our phone, whatever application is open, that's kind of like our registers, like the very things we're working with. And then we might have this set of most recently used apps as our cache. We can access them pretty quickly. You're straight to it open what we need. And the ideas that we've used them the most often, then we're probably going to open those apps again pretty soon. So we can get to them very quickly. And then if by some chance we need some other app, we can go to our entire shelf of apps and get whatever we need. And then if we really don't even have it on our phone, we can download it from the app store. And we could think of that as our hard disk or perhaps the internet, the network where we get our apps. So these hierarchies really affect us in our daily lives also. And so if we can apply that to our processor design, we'll make it more reflective of the programs that we're going to run. And it'll be more efficient in that. So with all this analog background, now we need to actually think of how we would implement a cache. And so if a cache is going to be the small subset of our main memory, we need to figure out what that subset is going to be was actually going to be contained in the subset of main memory in order to make it actually fast. And the other thing is that we do want to keep our clock period small. And so there will be different parameters that we can adjust in our cache that might increase the CPI in the clock period or vice versa. And we'll look at that as we go through a different cache configuration today. So for that, to sum up these analogies, we're going to use two ideas. And those ideas are called locality. There's temporal locality and there's spatial locality. So temporal locality is very analogous to the apps that I was trying to mount. So if you look on my phone, I use like discord, Instagram, YouTube, and maybe group me. I don't even use the texting app that often. So it probably won't appear in my most common sense. I'll use Chrome also. And so the idea is that if I open YouTube every day, then when I wake up in the morning, my phone's going to guess that I'll probably open it up again. And so it's going to have it right there, ready for me so I can open it up and not have to scroll from other apps because YouTube starts with Y and I have to scroll all the way down to the bottom. It was right there at the top. I can easily use it. And then I've saved that quarter second that it takes to scroll down. But those quarter seconds add up to other time throughout the day. So that's kind of the idea there. We also have this idea of spatial locality, which isn't really useful for apps, but maybe more in the library. So the idea of spatial locality is that if we use one piece of data, we're probably going to also use data that's around it. So we're thinking about this as a member of an array in particular. In computer science, we iterate through arrays all the time. We start the first one. And so if we start the first one, it is a pretty darn good guess that we're going to go to the next one. And then go through the entire thing. So if we can almost pre-load these other members of the array the first time that we access the array, then we won't need to go back to main memory later to access them again. And we've saved ourselves a lot of time on the level of how much we actually choose to load the first time. So for example, if I'm going through an array and when I access members zero of the array, if my processor loads like the first 16 members, then I've cut my access time about by 16 parts. So I only had to make that one memory access instead of all 16. So I save a lot of time. You can think about this with the library also. If you're familiar with the Dewey decimal system like libraries are organized very much by topic and then beyond that by author. So if you go access a book by one author, he might reference here, she might reference something else either from their own works or from the category where they would have their books sit in the library. So you can go to that same kind of location and find it. But if you grab all of that author's books the first time, then you don't need to do that later. Or perhaps if you're reading a series of books, if you're going through a five book series, why not just perhaps all of them at first. My sister does that a lot when she checks out books. She'll just check out like four of them. And she doesn't have to go to the library the next day when she inevitably finishes the first one at superhuman speed. So we can use these two ideas of temporal locality and spatial locality specifically with pieces of data in our processor and memory. And we'll see how we can use those to our advantage in order to optimize whatever this subset of memory that the cache represents. All right. Cool. So I guess I do want to talk about blocks. Blocks is going to be the main way that we take advantage of that spatial locality. So a block is going to be a chunk of data. Not a single piece. It's going to be a chunk of data that contains all pieces of data that are right there together. So perhaps in my analogy of the library, we're thinking about a series of books. You could also think of one book containing multiple pages. If each of those pages is a single piece of data, then the entire book itself is a block. And in our processor, those blocks, we have to make a decision as to how large that block is going to be. And so if we say, for example, that a block is going to be 256 integers long, so that's whatever 4 bytes sends 256 is, that's 1224 bytes. Well, if we only ever have arrays that are maybe 10 integers long, then we're going and fetching all this data that we don't actually need each time. But at the same time, if we make our blocks just one byte, and even when we access one integer, we have to make four memory accesses. So we're trying to find that sweet spot wherever the optimal size of a block is. Maybe it's the size of half of your average array. Maybe it's size of a full average array or something like that. And we're going to try to find that sweet spot for cache, but still make our ideas for cache and overall able to work with any block size. And so for this class, at least, all of our blocks are going to be powers of two. And that's because our blocks, we are able to take advantage of the binary number system that we talked about way back in discussion one. And we can use powers of two to break up addresses because every memory location, every piece of data memory has a unique address. We're able to kind of group addresses together if we use powers of two. Here's how that works. So let's say that we have this sample 16-bit address. And it's a bunch of ones and zeros. And remembering that memory is it is ordered sequentially. If we have this address and we change up just the last four bits, we know that whatever value we change those last four bits to, that address is still going to be close to the original because the first 12 bits here are going to be the same. So those two addresses return them into numbers. Those numbers are pretty close together. So that means that the actual pieces of data are going to be pretty close together. And so then if we call a block the set of all addresses that share the first 12 bits, then that means that block includes all 16 possible addresses that show these first 12 bits. And they vary in just those last four bits. So that's how we'll define a block. A block is just a set of sequential data and they share these same first bits which we're going to call a tag. A tag you can think if you're taking 281 sort of like a hash it's going to be a unique identifier for a block because no other block is going to have that same tag because all memory locations with that tag are in our block, which nothing else. So we can use our tags to uniquely identify our blocks. And then we can just use the last four bits to index into that block, which we can sort of represent as an array to get the precise piece of data whenever we're looking for it. And so we will come back to this when we start talking about virtual memory. But for now, I think of it as tags being the first bits that you need to identify block. And then our block offset these last bits are going to index into the block. So when we combine our block offset with our tag, we get a unique memory location. So if you're following along with the worksheet, which is available in the Google Drive, we're trying to figure out if we have a 16-bit processor with four bits for the block offset and 12 bits for the tag, how big are each of the blocks in the cache? And how many blocks can cache fit if we have 128 bytes of data search? So for the first one, that's going to be directly related to the number of bits in the block offset. So I said the block is going to tame all addresses where those last four bits vary. And how many possible values can those last four bits have? Well, that's going to be two to the four since we're in binary. So you have zero, which is zero, zero, zero, zero, and then one, which is zero, zero, one, it goes through two and two, f, if we're thinking in hacks or to 15, which is 1111 in binary. So all those are going to be inner block. And so that means that we would have a total of 16 bytes in our block here. And so we noticed that 16 is a power of two. So we can relate it to the four. So our answer here would be 16 bytes. But we get that knowing that 16 is equal to two to the power four, which is two to the power of our block offset bits, the number of log offset bits. All right. So knowing that we have 16 bytes in each block, if we have 128 bytes of data storage, we're trying to figure out how many blocks we have. Well, the easy answer there is just to divide 128 bytes total divided by 16 bytes per block. And that will get us, you know, to do the math here. Let's see, 16 times two, it's 32, 64128. I believe that's eight blocks. And here's the, I think I have the answers animated here. Yep. There we go. Yes, eight blocks is our answer. So we can do that easy division, just take our total cache size, divide by the block size, and that tells us the total number of blocks that we have in our cache. And that's pretty generalizable. So now we'll move on. Okay. So now we are given a bunch of 16 bit addresses for the same processor that we were working on with the first problem. And let's say that we have a theoretical cache with infinite blocks. If it's initially empty, given these addresses that we have on the bottom here, how many memory accesses do we need in order to perform reads at the following addresses? So, and this question kind of sinks from the idea that each time we do a memory access, we're loading a full block. So I pretty much the question is how many distinct blocks are we accessing? And so first we have to figure out how many blocks or which blocks are represented by our addresses. And if we're using the same 16 bit format, we remember that the blocks are uniquely identified by the tag. So now we just have to figure out how many tags are. So let's look at our addresses here and figure out what tags are represented here. So starting with 0, 0, 0, f, if we remember that four bits is equivalent to one hex digit. And we know that the last four bits is a block offset. We'll go ahead and chop off the f here as our block offset. That means that 0, 0, 0 is our tag. So we'll go through each of these addresses and do the same thing. So BEAs are tag here, DEAs are tag here and here and here, 1, 3, 3 is our tag here, 0, 0, 0, pops up again here and then BEE is our tag for the last one. And so I mean that f, 7, D, 0, 7, 1, 0 and f are all of our block offsets respectively. And so how many blocks are represented here? We have 0, 0, 0, BEAs, 2, DEA, and DEAs repeated. So we don't count that. 1, 3, 3, that's new. DEA, that's a repetition, 0, 0, 0, that's a repetition, and then BEE. So that means we have a total of five blocks. So that will mean a total of five memory accesses for these addresses that we're accessing. All right, let's move on here. So now we're thinking, so if our cache is a subset and we have to make sure that at some point we can access everything memory or also our program just straight up doesn't work. That means that we have to remove things from the subset of memory sometimes in order to fill it up with other things. And there are questions, as to what is the best policy for eviction. But in this class, we're going to use this idea of least recently used. So the idea is that if we have a piece of data that hasn't been used in a long time, we're probably unlikely to use it again in time soon. And so we'll take that out of our cache to replace it with something that will be accessed more often. And if we're wrong, that's okay, we'll recover. And so there are other ideas that you use. Like maybe you make a cache where you have like a least often used heuristic where you like have a counter in each cache block, which is the number of times that it's been accessed and so it's first put in the cache. Maybe you would like those giving a little priority to least recently used. But that's the policy that we're going to use in this class, least recently used. And so if we think about our phone, let's say by some tragedy that I stop using Instagram, if I stop using Instagram day after day and I use it less and less, then my phone will I think probably the next day I'm not going to use Instagram yet again. And so it'll replace it with something else that's more relevant. Maybe at that point, you'll replace it with a settings app. Who knows. But that's the idea of least recently used. Right. So now we're going to go through the same addresses from problem two, four problem three. And we will figure out what block based on each access is our least recently used, which will tell us which block we can evict from our cache, take out of our subset whenever we need to bring something you in. All right. So let's see. So just starts off, we do have to know how we're going to track our LRU. And we're going to track our LRU, which is simple numbers. So knowing that, let's see, going back, let's see, the promises that are cache only have four blocks. So that means that each block will get a unique number to tell us to give us like an ordering of which is least recently used. And so for this problem, I'm going to say that zero represents least recently used and larger numbers represent more recently used. So we'll go through and see what that looks like. So but before we do that, we do know that if each block has a distinct LRU like a field or maybe a tag or something, then since we only have four blocks in our cache, we only need four distinct LRU values. And so that means that we will just use two bits to represent those four values, zero, one, two, and three. And so if you note two, the two bits that we're working with, two is the log base two of four. So if you ever have more blocks in cache, like eight, for example, you can just take the log base two of eight to figure out how many LRU bits mean. And when we get into sets, that will be allocated more on a set level instead of a larger block level or cache level. So if you have a set with four blocks, then you would only need two LRU bits for each block. Okay. But let's actually go through the problem now. So to answer the question, we have two LRU bits, it's two log base two of four blocks. So that means that each of four blocks, so two bits total of eight bits, it's probably not really relevant. Let's go through the accesses now. All right. So first access, and I'm going to remind us by drawing this line between our tags and block offsets, so we know which blocks we have. So our first access is to block zero. And so I said that zero, zero was the least recently used. So that means that when I first access block zero, it's going to be our most recently used. And so we'll give it an LRU value of one one, which is three. Okay. So then we'll access BEA as our second block. And now BEA is our most recently used. So we're going to give it an LRU value three and decrement the LRU value of block zero down to two. All right. Moving on, we're going to access block DEA. DEA and same as before, we're going to give it a most recently used marker. And then we'll decrement the other markers to to what they should be. So that means that BEAs are second most recently used. So we'll give it a value two, one zero, and block zero is our least recently used at these three. So we'll decrement that one down to one. Right. And then we'll do the same thing. We'll access block one, three, three. Give it the most recently used marker, which is one one, and decrement all the others. All right. So I'm just saying to write that down. So that means that DEA has LRU marker two. BEA has LRU marker one, and block zero has LRU marker zero. So block zero is still our least recently used. We know that just by looking at the program, if you look at the top, we haven't access that block zero since the very beginning, then it's our least recently used. All right. So now it will get tricky. We're going to access DEA, which is already in our cache. So we're going to call that cache hit. We don't have to make room for anything or fill up an empty block or anything. But we are going to update it to be the most recently used. So we're going to have to give it an LRU value of two. And so same as before, we're going to decrement the others. So sorry, I meant to say DEA will get three. So we'll give one three three of value of two. But if we were to decrement BEA, that would go down to zero. If we were to decrement zero zero zero that would go down to negative one, which doesn't really make sense for us if we think of zero as being the least recently used. So we're not going to actually decrement those. And when you're doing your project, you're going to have to make this decision. And I think the easiest way to make the decision is when you get a cache hit, you save what the original LRU value was. In this case, it was one zero. And then you only decrement the blocks that are more recently used than that block that you got to hit on. So if DEA had an LRU value of one zero, then we would only decrement blocks with LRU values higher than one zero. In this case, that was just one three three. And that would exclude BEA in zero zero zero, since their LRU values were less than the DEA value here. And so that's kind of algorithm that we'll use. So let's practice that one more time. Let's say that we access block zero once again. So I'm going to look at the original LRU value. In this case, it's zero zero. And I'm going to save that. And then I'm going to give block zero an LRU value of the maximum, which is most recently used, which is one one. So put that there. And then we'll decrements any LRU values that are higher than zero zero, which coincidentally is everything. So this time, we'll just end up documenting everything once again. So that will mean that the first one, BEA will go from zero one to zero zero. So now BEA is the least recently used. The DEA will go from one one to one zero. One three three will go from one zero to zero one. All right. Last access is to block B. And now our cache is full. So we actually need to evict something. We need to make room for block B. And so we will evict whatever was least recently used, which here is BEA. So we'll evict BEA, bring in block BEE instead and give BEE the most recently used marker, which is one one. And then we'll decrement all the others this time. So you only need to do the selective decrement if in fact you get a cache kit. All right. So BEA is the most recently used at one one zero zero was our second most recently used. And then it was the DA. So BEA is our one. And then one three three will be zero zero for all of you that's. And that's the final state of our LRU here. All right. So we will move on from LRU to other things now. Okay. So not only do we need to be able to read from memory, we also sometimes need to be able to write to memory. And it's not as common as reading from memory, but we still need to do it sometimes. So this is not true in the library. When you're in your library, you do not hopefully write in the books. That would be bad. But in real processor world, we do need to actually write to memory sometimes. So we have a couple things that we can do to figure out how to make it all work. Because of course, if we're writing to memory, the idea is that we'll read from that location later. And so we want to make sure A that it's actually the correct value that we're writing to memory. So if we if we're working with both our memory and our cache, we want to make sure that there's some sort of consistency model there. So when we access that address again, we're getting the correct value based on our program. And the other thing is that we do want it to be fast. We don't want it to be just super slow. So those are the two things that we're working with here. So the first idea that we have is to have a write through cache. And so that means that anytime we have a write, which in else to case, an s sub instruction, we're going to update that value in the cache at that address. That is if it's in the cache, depending on if it's allocated or not, until it's like about a minute. And then if we're writing through, we'll go straight through the cache and also update memory. And there's an argument as to whether or not this is fast. I think I think it's not considered very fast. Nowadays, just based on statistics. But if stores to memory don't happen very often, then we can first write the cache. And then we can have memory going update in the background. And if we keep doing our reads from the cache, then we don't really need to worry about waiting for memory to finish up that store before we go access memory again. Now, is that actually possible? Most besides just straight up tests from all of our benchmarks, I think it's been said though that that's not really feasible. That we do read from memory so much because even with a good cache, we still get cache misses to where we need to read from memory that writing back or writing to memory in the background isn't really possible because it always slows down and hinders reading from memory again. But there's different arguments we've made there and who knows that the technology changes in however many years. Anyway, so we can visualize this for writing some piece or for writing some value to unaddress, which would be data here, data would slip, address, then we would look it up in our cache, update it in our cache first. And then we would also go to memory and update that same address. So that's the idea. The weird thing is that these addresses don't match up, but I'm betting that's based on tags of using a strange number of tag bits. We'll move on. The other option is to only update the cache whenever we make a write and then our cache sort of becomes like an abstraction of memory. So whenever we go to what we think is memory, we'll first check the cache and then if we miss we'll go to memory. And so if our cache is the first thing that we check, then we'll always get the correct value if it's in the cache. And then if it's not in the cache, then it's in memory and so we'll get the correct value anyway. But in order to maintain that consistency, if a block is ever evicted from the cache and that has to be written back to memory, but otherwise we don't have to write it back. And so we are going to maintain this consistency by using dirty bits. So the idea is that if we only read a block, then we're not changing it. So when it's evicted, we don't need to write it back to memory. This is just writing it back with its original value that does nothing for us. So we will only write back blocks with changed values. We're going to call them dirty. We'll write those back to memory and that will reduce the number of times that we write to memory. And it will be consistent, which is cool. So those are the two options. For the project, we are going to be using writeback cache. So that means you should only be storing two memory when you're performing an eviction of a dirty block. We should have no other lines of code that's write to memory. So there's a big hint for the project. And writing to memory in project 4, it's going to be a function call. And so you should only see a function call. That's a write pretty much in one line. When you're evicting a dirty block, that's the only time. I did mention allocation. We also have within each of writeback and write through two options. We have the option to allocate on a store, which means basically do a load from memory to the cache before we edit the data in the cache. Or we can do no allocation. So if we do a store look up in our cache, if it's not there, then we'll just write it straight to memory. But if it is there, then we'll write to the cache. And if we're doing write through, we'll still write to both. So let's look at the four options. So if we're not allocating stores, we're doing a writeback cache, then we will look up, whenever we do a store, we'll look it up in the cache. If it's there, we'll edit the cache and be done. If it's not there, then we'll edit to memory and we'll be done. And then of course, if we ever evict a dirty block from the cache, it'll write back to memory. If we're allocating, then whenever we do, or if we're going to write back policy with that allocate, if we do not find our piece of data in the cache, we'll first load it from memory to the cache and then edit it in the cache. But if we're doing write through with store allocate, we'll essentially load from memory and then write to both memory in the cache. So that's kind of probably the most redundant since we are editing a block in the cache twice, which doesn't really make sense. If we're just going to write it on the second time. But each of these is a good consistency model. For this class, though, we are going to use write back caches and we are going to allocate upon a write. So that means again, whenever we get a store miss, we'll read it from memory first and then edit it and then we won't have to write it back to memory until it's evicted. So now we'll move on to looking more at sets. And this will be related closely to the data structures in the project. We've given you a bunch of star structs, just like project 3 and the other projects. And so we're going to look at the things that are in those structs with that actually looking at the code. So the sort of we're going to look at the things in a block struct. And so blocks are also called cache lines, by the way, so that's equivalent terminology. So the data structures that we need to represent a block are as follows. So we have the tag. I said that before a tag will uniquely identify a block with an exception that will come in a minute. And then we of course have the data. We obviously need that. And that's the size of this data is going to vary based on our block size, obviously. And then we'll have a couple of other things. So I did say that our LRU will be also allocated on a per block basis. So within a set, each block is going to have a distinct LRU value. And then in order to track the store stuff will have one dirty bit per block. So that means that if we ever evicted block, we will write back the entire contents of the block instead of writing back the exact bytes that were changed. Because if we were to write back the exact bytes that were changed, then we'd have to maintain a dirty bit for every byte in the block. So it's going to be easier for us to just track dirty nests on a block level entirely. And then finally, we'll have a valid bit because caches are going to start empty. And in hardware, everything's going to be either zeros or ones. So if we were to look in our cache, thinking that it's empty, we would see a bunch of zeros and think that that might be valid data when we're looking for a cache hit. And so we have to explicitly mark it as not zero, I suppose, whenever we start up our cache. So we're going to have a valid bit to maintain whether or not it's actually data or just whatever was in the cache at the beginning. So those are all the things that you'll find in your block struct for project four. It's all on the start code. Before we look at the other data structures for the cache, let's do a little review. So this project is going to be heavily dependent upon all the binary stuff that we did, discussion one. So remember that if we're choosing among a power of two number of items, then we can uniquely identify each of those items with whatever that exponent is number of bits. So if we're trying to uniquely identify 16 different items, we take the log base to 16, which is four. And then that means that we only need four bits to uniquely identify each of the 16 items. So that's going to motivate us here. So another example, if we have, let's say, 64 unique blocks, perhaps, that means that we need the log base to 64, which is six. We need six bits in order to uniquely identify those blocks within our cache, not from other blocks in memory, but within our cache, we need 16 bits. So I'm not talking about tags, just move like an index here. And then other things as review, a kill byte is equivalent to two to the power of 10 bytes, or 1024 bytes. And megabyte is two to about 20 bytes, which means that a megabyte is also equivalent to two to the power of 10 kilobytes, because of exponent roles. And then a gigabyte is two to about 30 bytes, which means that it's two to the power of 10 megabytes, or two to about 20 kilobytes. And so we're going to be using this math quite a bit in order to simplify some of our calculations when we don't want high components. So for example, if we have 32-bit addresses, which is common of most processors before say 2010 or so, then we have 32 bits to uniquely identify each byte in memory. And so that means that there are a total of two to the power of 32 memory locations, which means that we can do our exponents here. So we know that two to the power of 32 is equivalent to two to the power of 22 kilobytes, or two to the power of 12 megabytes, or two to about two gigabytes, which is four gigabytes. And if you're tracking along here, that's why most processors have moved on to 64-bit addresses now, because if we want to uniquely identify all the pieces of data in more than four gigabytes, then we need more than 32 bits in an address. So I know Mac OS a couple of years ago now, maybe two years ago now, switched to 64-bit only support. And I think Windows is kind of moving in the same direction. And so that's why. That's why, because if we have 32-bit addresses, we can only uniquely identify the data in up to four gigabytes. And it's based on all this math, which is cool. All right. So we have to now think about the difference between hardware and software. And they're going to be like slightly related, but we have to think about lookup time now in terms of parallelism versus sequential access. Luckily, the ideas are going to be pretty similar. So let's say that we have those 128 blocks. So we have a cache with 128 blocks. We're searching for just one piece of data. Now, if we were to go through each block one by one and look for cache hit, that would take a while. So if we're thinking about in two 81 terms, even if we're doing linear access time, we still might take a long time if the size of the array is just really, really long. And so if we can figure out like a smaller range where our address or piece of data might be, then we can just look in that range and greatly reduce our access time on the order of some power of two, let's say. So if we have an array and we know which core to the array our final piece of data is in, then we can only look in that quarter and we've cut our access time for. Now, that's really not true when it comes to hardware because everything's going to be in parallel. And when we're looking in parallel, it's going to be a bunch of mixes and a quality checker, which is like a bunch of x and or gates. And if those are all happening at the same time and then they're all coming together in a bunch of or gates, then it's going to increase our propagation delay for the entire cache, but it's not really going to be like on the level of a power of two. It's going to be more linear at that point. So if you're thinking about this in terms of arrays, it's definitely going to be power of two. But if we're thinking about hardware specifically, what it actually takes to figure out if we have a cache hit, it's going to be very different. But if you want to learn more about that, go take x470 because we do a lot of that stuff in x470. But still, because we still have this propagation delay, it would still be nice if we can reduce the amount of blocks that we have to look at in order to find our final data. And so in order to restrict that, we're going to take all of our blocks and we're going to divide them up into different sets. And we're going to relay our sets to the addresses of what we're looking for. We're going to insert some bits. And I say insert, we're really going to divide up the last of our tag bits. And we're going to designate those as set bits. So whenever we get an address, we can immediately look at those set bits and know which of the sets in our divided up set of blocks that we are going to look at. And so that means whenever we need to bring a new block into the cache, now our block is going to be uniquely identified by the tag concatenated with the set bits. And that block will always go to the same set, no matter if it's big bit and if it comes back and whatever, it's always going to go to that same set, but it might go to a different block in that set depending on the eviction pattern. So to review a little bit before we look at how the set is implemented, we started off with a fully assertive cache, which is essentially a cache where the entire thing is one set because you have to look at every single block in order to figure out if you have a hit. And then you look at all the blocks to figure out which one has the lowest LRU value to evict. And so that takes quite a while in order to do both those things. But in theory, that's going to give us our best LRU policies because the exact block that is least recently used at the entire cache will be evicted if we need to bring in something new. On the other hand, we have something called a direct map cache, which is a set associated cache where each block is its own set. So that means that we have a bunch of set bits and we don't have to do much look up at all. Whenever we're looking for a cache hit, we just take the set bits from whatever address we're looking for. And then we look at that block, that single block, and check if the tags match. If they don't, then we'll have to evict and bring in the block that we're looking for. And if they do match, then it's that one block. We don't have to look at any other blocks. There's no such thing as LRU because if we ever evict, it's just that one block that we have to evict. And so that's going to be our direct map to cache, direct map meaning the mapping directly from set bits to this single block in that set. The only thing is that if we are constantly accessing two blocks that have the same set bits, they're constantly going to be evicting each other left and right. And so it might greatly increase our average minimum access time, increase our CPI and all those things. So we can most of the time find something in the middle where we have a set of search of cache. And technically the full use search of cache and the direct map cache are still set caches. But we will generally call the middle ground the set of search of cache. And so we can index into our array of blocks based on the set bits. And then only look at the few blocks step along to that set. And so the number of set bits is going to be the log base two of the number of sets that we have. So if I have one set bit or the only values for one bit number or zero one, so we'll have sets zero and set one. And I'll be it. And so like I said before, we're going to track L or U within the set. And so whenever you need to evict, you'll evict from the set that ungrace belongs to. So when you bring in the new block, you know that that block is in its set that it's supposed to belong to. It's not evicting some other set. All right. So let's just kind of recap all them. You can see them side by side in this picture here. We have our set of search of cache which is broken up by sets. And then the full use search of cache is a special type of set of search of cache, which is one set, which means that there's no set bits. And the number of blocks in the cache are the number of blocks in this one set, obviously. And then for the direct map cache, the number of sets is equal to the number of blocks because the number of blocks in set is just one. It was the set and block at that point by the same. By the way, a block inside a set, the number of blocks for each set is also called the number of ways because whenever we bring in something new, there's, you know, that many ways that it could go, that many possible destinations that we could bring it into. And so yeah. Right. So let's look at an example. If we have an address of 1,001, 0111 in binary, we're trying to figure out where would go in each of our three cache configurations, therefore working with two byte blocks with eight byte caches. So of course, that means that we will have four total blocks, is that eight bytes divided by the two bytes is four blocks. So if we have a four block cache with these different configurations, where will this address go if we're bringing it in for the first time? So let's look at that. So if we have a fully assertive cache that we assume to be empty at the beginning, let's say just for the example that we're going to put our new block at the top. So in that case, we would put a new block just directly in the top one. We don't even have to look at the address to know where it's going to go. For the set assertive cache, we do have to look at the set bits. And I have to go back here. So we have two byte blocks. So that means we have the log base two of one, or sorry, the log base two of two, which is one number of set bits. So this block offset bits, mixing in my words, here's the one here is our block offset. And so we have two sets based on the picture here. So that means we have one set bit. And so that means that our new address, our new block is going to go into set one. So set one's empty. So we'll just go to the top empty block and set one and put our new block there. Finally, for our direct map, we see that there are four blocks, which means that there are four sets. So we take the log base two of four, which is two. That'll be the number of set bits that we have. So going back to our address, besides the block offset at the end, those last two bits are going to be our set bits for our direct map. And so that's a one one, which is three. So our new block will go into set three, which is block three. And it'll go right there. And so that's where a block would go, depending on our different cache configurations. Now, so this is a visualization here, if you're watching and not listening, of the actual structs that will be in our project. And so we're going to give you the opportunity to track all these different things. So you can track the tag in your block struct. You don't need to track the block offset bits, since those are really into like an index into the actual words that are in your block. So you don't need to track that explicitly. And then you'll have a set bits also, which again, you don't need to track explicitly, you can just use that as an index into the entire cache to figure out where your set is. And so if we look at now this picture here, I want to show you how you would look at that. So of course, if you have a fully assisted cache, we're going to give you a cache struct that has an array of blocks. So each of those blocks will be this other struct with all these things that we're talking about before. But if you have a fully assisted cache, then in your code, you'll go through the entire array of blocks every time you're looking for a hit or an every time you're looking to update LRU. And then if you have a direct map cache, then you won't have to look through the array at all, you'll just use the set bits, the block bits that is as an index into your block array. And look at that one spot to check for a hit. But if you have a set assistive cache, then it gets a little tricky. So if we have say a set assistive cache with two cache with two sets and each set has two blocks, and that means that block 0 and 1 will belong to set 0 and block 2 and 3 will belong to set 1. And so you can do a little math here, you can do some multiplication left shifting. So if we have two blocks per set and we're looking at say set 1, then we'll skip those first two blocks and set 0. So we can multiply our 1, our set index by the number of blocks and that will go past all the previous blocks that belong to previous sets. And so we can start our array access at the number of blocks per set times our set index. And then we can go to that value. So the number of blocks per set times our set index plus the number of blocks per set minus 1. And that that closed interval that is will contain all the indices for the actual blocks that we're examining when we're looking to find hit or maybe update our you. So you can definitely use that in a project. And the cool thing is that this works out with our fully associative and direct map caches. You don't need separate if statements to check if it's a fully associative or direct map catch. Because of course, if we have a fully associative cache, you'll always be going to set 0. And so that means that you'll just start at index 0 since zero times block per set is 0. And then you'll go to the number of blocks per set minus 1. So you'll look at every block in your cache, which is which one. Now if you have a direct map cache, just to know that it works with the same thing, that means our blocks per set is just one. And so then whatever our set index is, the number of blocks per set times the set index is equal to that value plus blocks per set, which is 1 minus 1. So if you add one, track one, this is the same thing. And so you'll look at only that one index, which represents your tire set, because it's just that one block. So I highly, highly, highly recommend that you use that approach for the project, because we've given you the starter structs, which are organized that way. We have essentially a 2D array going on. So we have our cache, which has an array of blocks. Each block has an array of data. And so if you wanted to, you could make a cache that has an array of sets, and each set has an array of blocks, and each block has an array of data. But once you get to that large of a struct, it's going to take a lot of memory on your actual project C code. So I don't recommend you do that. I was going to be much more representative of the hardware also, if you use the 2D array that was given you in the starter code. So highly, highly, highly recommend that. And that's how all the iAs did it. So they're going to be much easier to help with that approach in office service instead of trying to make a 3D array struct. Because believe me every semester, we see that the 3D array causes a bunch of set faults, and it's super, super hard to debug. So use the 2D array method from the starter code. That's my suggestion. All right. So I think this will conclude sort of our discussion from today. This slide is actually incorrect. I meant to put this on next week's slides. But next week we'll be back. I think the next two weeks actually will be continuing to talk about caches. And so we'll look at a problem next week, or maybe in two weeks. I forget, of an actual cache from an Intel i5 CPU. And so there's a whole bunch of strategies that we can employ. So we can do all of our right policies, our allocate policies. We could use something besides LRU if we wanted to. It might be pretty difficult. There's multiple ways to tag caches. So sometimes you can use some sort of hash algorithm to determine sets instead of just a regular set index. So there's a bunch of things that you can do. If that sounds really cool, will you go take eeks for 70? And but make sure you know at least the basics for this course. We're not going to really test on hash caches or multi-level caches or all these different things in this course. So make sure that you understand the project. Because I think for the exam project 4 is the most relevant to either the exams. That's just my opinion. And so if you have any questions, post on Piazza, go to office hours. Don't be afraid because it is a difficult concept you've never heard of a cache before. Like when I came to 370, I'd like heard of a cache. And I thought it was just like a file on my phone. I had no idea that these things existed in all of our processors. And so if you do know that, you know that now. And so don't be afraid to come to office hours. And by the way, here's a bunch of important formulas. You can pretty much screenshot this and put this on your cheat sheet for the final. But you have to know what they actually mean in order to, you know, do well in the problems. But I would also recommend using this on your projects. Because some of these are going to be very useful. So that's everything I have for today. Good luck with project 4. And if you have any questions, put them on Piazza in the discussion folder. And we'll get back to you soon. See you later, everyone.